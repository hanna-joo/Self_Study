{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Categorical Feature Encoding Challenge Step 4","metadata":{"papermill":{"duration":0.021557,"end_time":"2021-07-31T03:06:28.409888","exception":false,"start_time":"2021-07-31T03:06:28.388331","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"---\n### Analysis summary and modeling strategy\n#### 1) Analysis summary\n1. No missing values\n2. No features to remove\n3. Encode binary features : Change values to 0 and 1\n    + **'bin_3'** and **'bin_4'**\n4. Encode nominal features : One-hot encoding because the data quantity is not that much\n    + **'nom_0' ~ 'nom_9'**\n5. Encode ordinal features : Encode as the order of unique values\n    + **'ord_0' ~ 'ord_5'**\n6. Encode cyclical features : One-hot encoding to prevent recognition as large or small values\n    + **'day'** and **'month'**\n\n#### 2) Modeling strategy\n- Baseline model : Logistic Regression\n    + Feature engineering : One-hot encoding of all features\n- Performance improvement : Additional feature engineering and hyperparameter optimization\n    + Feature engineering : <u>Custom encoding for categorical features</u> and <u>feature scaling</u>\n    + Hyperparameter optimization : GridSearch\n    + Additional tip : Use validation data for training\n--- ","metadata":{}},{"cell_type":"markdown","source":"## 4. Performance improvement\n- Process\n    1. Import data\n    2. Feature engineering\n    3. Make evaluation metrics calculation function\n    4. Hyperparameter optimization (Train model)\n        + **Generate model**\n        + **Generate GridSearch object**\n        + **Train GridSearch**\n    5. Validate performance\n        + If performance is not good, go back to **Feature Engineering** or **Hyperparameter Optimization**\n    6. Submit","metadata":{}},{"cell_type":"markdown","source":"### 4.1. Modeling focused on features\n- Key point\n    + **Custom encoding** for categorical features\n        + Binary features : Manual encoding\n        + Ordinal features : Manual encoding and ordinal encoding\n        + Nominal features : One-hot encoding\n        + Date features : One-hot encoding\n    + **Feature scaling** for ordinal features\n    + Hyperparameter optimization","metadata":{}},{"cell_type":"markdown","source":"#### 1) Import data","metadata":{}},{"cell_type":"code","source":"# Import data\nimport pandas as pd\ndata_path = '/kaggle/input/cat-in-the-dat/'\ntrain = pd.read_csv(data_path + 'train.csv', index_col='id')\ntest = pd.read_csv(data_path + 'test.csv', index_col='id')\nsubmission = pd.read_csv(data_path + 'sample_submission.csv', index_col='id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2-1) Feature engineering : Custom encoding for categorical features","metadata":{}},{"cell_type":"code","source":"# Merge train data and test data\nall_data = pd.concat([train, test])\n# Remove target values for separately modeling feature and target values\nall_data = all_data.drop('target', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### (1) Binary feature\n- 'bin_3', 'bin_4' : Manual encoding","metadata":{}},{"cell_type":"code","source":"all_data['bin_3'] = all_data['bin_3'].map({'F':0, 'T':1})\nall_data['bin_4'] = all_data['bin_4'].map({'N':0, 'Y':1})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### (2) Ordinal feature\n- 'ord_1', 'ord_2' : Manual encoding\n- 'ord_3', 'ord_4', 'ord_5' : Encoding as the alphabetical order","metadata":{}},{"cell_type":"code","source":"# Manual encoding\nord1dict = {'Novice':0, 'Contributor':1,\n            'Expert':2, 'Master':3, 'Grandmaster':4}\nord2dict = {'Freezing':0, 'Cold':1, 'Warm':2,\n            'Hot':3, 'Boiling Hot':4, 'Lava Hot':5}\nall_data['ord_1'] = all_data['ord_1'].map(ord1dict)\nall_data['ord_2'] = all_data['ord_2'].map(ord2dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding as the alphabetical order using OrdinalEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\n    # List to encode\nord_345 = ['ord_3', 'ord_4', 'ord_5']\n    # Generate encoder object\nord_encoder = OrdinalEncoder()\n    # Apply ordinal encoding\nall_data[ord_345] = ord_encoder.fit_transform(all_data[ord_345])\n    # Print encoding order by feature\nfor feature, categories in zip(ord_345, ord_encoder.categories_):\n    print(feature)\n    print(categories)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Before ordinal encoding\ntrain[ord_345].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After ordinal encoding\nall_data[ord_345].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### (3) Nominal feature\n- 'nom_0' ~ 'nom_9' : One-hot encoding\n- Note\n    + When applying one-hot encoding with OneHotEncoder, sparse matrix is returned as CSR(Compressed Sparse Row) format\n    + CSR format uses less memory and makes calculation speed faster ","metadata":{}},{"cell_type":"code","source":"# One-hot encoding\nfrom sklearn.preprocessing import OneHotEncoder\n    # List to encode\nnom_features = ['nom_' + str(i) for i in range(10)]\n    # Generate encoder object\nonehot_encoder = OneHotEncoder()\n    # Apply one-hot encoding\nencoded_nom_matrix = onehot_encoder.fit_transform(all_data[nom_features])\nencoded_nom_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove nominal features from all_data\nall_data = all_data.drop(nom_features, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### (4) Date feature\n- 'day', 'month' : One-hot encoding","metadata":{}},{"cell_type":"code","source":"# One-hot encoding\n    # List to encode\ndate_features = ['day', 'month']\n    # Apply one-hot encoding\nencoded_date_matrix = onehot_encoder.fit_transform(all_data[date_features])\nencoded_date_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2-2) Feature engineering : Feature scaling\n- When numerical features have different valid value ranges, training won't be done well\n- Feature scaling : The operation of adjusting the range of values of features to match each other\n    + Binary, nominal, date features are encoded 0 and 1\n    + The value range of the ordinal feature must also be scaled to be between 0 and 1","metadata":{}},{"cell_type":"code","source":"# Feature scaling\nfrom sklearn.preprocessing import MinMaxScaler\n    # List to scale\nord_features = ['ord_' + str(i) for i in range(6)]\n    # Just to compare before and after\nbefore_scaling = all_data[ord_features]\n    # Min-max normalization\nall_data[ord_features] = MinMaxScaler().fit_transform(all_data[ord_features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Before scaling\nbefore_scaling.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After scaling\nall_data[ord_features].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge encoded and scaled features with csr format\nfrom scipy import sparse\n\nall_data_sprs = sparse.hstack([sparse.csr_matrix(all_data), # Return all_data to CSR format\n                              encoded_nom_matrix,\n                              encoded_date_matrix],\n                              format='csr')\n\n# Split data into train data and test date\nnum_train = len(train)\nX_train = all_data_sprs[:num_train]\nX_test = all_data_sprs[num_train:]\ny = train['target']\n\n# Split train data into train and validation data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y,\n                                                     test_size=0.1,\n                                                     stratify=y,\n                                                     random_state=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3) Make evaluation index calculation function\n- Use scikit learn library\n    + sklearn.metrics.roc_auc_score","metadata":{}},{"cell_type":"markdown","source":"#### 4) Hyperparameter optimization (Train model)\n- GridSearch finds **optimal hyperparameter values** by changing hyperparameter values and evaluating model performance through cross-validation\n\n\n- Process\n    1. Generate model\n    2. Generate GridSearch object\n        + Target model\n        + List of hyperparameter values (Dictionary type)\n        + Evaluation function for cross-validation\n    3. Train model (GridSearch)\n    \n\n- Note\n    + C : A parameter that controls the intensity of regulation and the smaller value, the stronger regulation","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n# Generate model\nlogistic_model = LogisticRegression()\n# List of hyperparameter values (Dict type)\nlr_params = {'C':[0.1, 0.125, 0.2], 'max_iter':[800, 900, 1000],\n            'solver':['liblinear'], 'random_state':[42]}\n# Generate GridSearch object\ngridsearch_logistic_model = GridSearchCV(estimator=logistic_model,\n                                        param_grid=lr_params,\n                                        scoring='roc_auc',\n                                        cv=5)\n# Train model and GridSearch\ngridsearch_logistic_model.fit(X_train, y_train)\n\nprint(f'best hyperparameter :', gridsearch_logistic_model.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5) Validate performance","metadata":{}},{"cell_type":"code","source":"# Predict the probability of target value 1 with validation data\ny_valid_preds = gridsearch_logistic_model.predict_proba(X_valid)[:, 1]\n\n# Validate model\nfrom sklearn.metrics import roc_auc_score\n\nroc_auc = roc_auc_score(y_valid, y_valid_preds)\nprint(f'ROC AUC of validation data : {roc_auc:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6) Submit","metadata":{}},{"cell_type":"code","source":"# Predict with test data\ny_preds = gridsearch_logistic_model.best_estimator_.predict_proba(X_test)[:, 1]\n\n# Save submission file\nsubmission['target'] = y_preds\nsubmission.to_csv('submission.csv')\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2. Train all data including validation data\n- Training with even a little more data is beneficial for performance improvement\n- Key point for performance improvement\n    1. Do feature engineering (Encoding/Feature scaling)\n    2. Do modeling using different kinds of models and hyperparameter optimization\n    3. Select one model whose performance of validation data is the best\n    4. Train the selected model again with full training data including validation data\n    5. Submit","metadata":{}},{"cell_type":"markdown","source":"#### 1) Import data","metadata":{}},{"cell_type":"code","source":"# Import data\nimport pandas as pd\ndata_path = '/kaggle/input/cat-in-the-dat/'\ntrain = pd.read_csv(data_path + 'train.csv', index_col='id')\ntest = pd.read_csv(data_path + 'test.csv', index_col='id')\nsubmission = pd.read_csv(data_path + 'sample_submission.csv', index_col='id')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:18:23.504729Z","iopub.execute_input":"2022-04-26T14:18:23.505586Z","iopub.status.idle":"2022-04-26T14:18:25.576609Z","shell.execute_reply.started":"2022-04-26T14:18:23.505324Z","shell.execute_reply":"2022-04-26T14:18:25.575713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2-1) Feature engineering : Custom encoding for categorical features","metadata":{}},{"cell_type":"code","source":"# Merge train data and test data\nall_data = pd.concat([train, test])\n# Remove target values for separately modeling feature and target values\nall_data = all_data.drop('target', axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:18:25.578609Z","iopub.execute_input":"2022-04-26T14:18:25.578846Z","iopub.status.idle":"2022-04-26T14:18:26.414268Z","shell.execute_reply.started":"2022-04-26T14:18:25.578819Z","shell.execute_reply":"2022-04-26T14:18:26.413368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Binary feature\nall_data['bin_3'] = all_data['bin_3'].map({'F':0, 'T':1})\nall_data['bin_4'] = all_data['bin_4'].map({'N':0, 'Y':1})\n\n# Ordinal feature\nord1dict = {'Novice':0, 'Contributor':1,\n            'Expert':2, 'Master':3, 'Grandmaster':4}\nord2dict = {'Freezing':0, 'Cold':1, 'Warm':2,\n            'Hot':3, 'Boiling Hot':4, 'Lava Hot':5}\nall_data['ord_1'] = all_data['ord_1'].map(ord1dict)\nall_data['ord_2'] = all_data['ord_2'].map(ord2dict)\n\nfrom sklearn.preprocessing import OrdinalEncoder\n    # List to encode\nord_345 = ['ord_3', 'ord_4', 'ord_5']\n    # Generate encoder object\nord_encoder = OrdinalEncoder()\n    # Apply ordinal encoding\nall_data[ord_345] = ord_encoder.fit_transform(all_data[ord_345])\n    # Print encoding order by feature\nfor feature, categories in zip(ord_345, ord_encoder.categories_):\n    print(feature)\n    print(categories)\n    \n# Nominal feature\nfrom sklearn.preprocessing import OneHotEncoder\n    # List to encode\nnom_features = ['nom_' + str(i) for i in range(10)]\n    # Generate encoder object\nonehot_encoder = OneHotEncoder()\n    # Apply one-hot encoding\nencoded_nom_matrix = onehot_encoder.fit_transform(all_data[nom_features])\nencoded_nom_matrix\n    # Remove nominal features from all_data\nall_data = all_data.drop(nom_features, axis=1)\n\n# Date feature\n    # List to encode\ndate_features = ['day', 'month']\n    # Apply one-hot encoding\nencoded_date_matrix = onehot_encoder.fit_transform(all_data[date_features])\nencoded_date_matrix","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:18:26.415683Z","iopub.execute_input":"2022-04-26T14:18:26.416028Z","iopub.status.idle":"2022-04-26T14:18:29.612136Z","shell.execute_reply.started":"2022-04-26T14:18:26.415985Z","shell.execute_reply":"2022-04-26T14:18:29.611318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2-2) Feature engineering : Feature scaling","metadata":{}},{"cell_type":"code","source":"# Feature scaling\nfrom sklearn.preprocessing import MinMaxScaler\n    # List to scale\nord_features = ['ord_' + str(i) for i in range(6)]\n    # Just to compare before and after\nbefore_scaling = all_data[ord_features]\n    # Min-max normalization\nall_data[ord_features] = MinMaxScaler().fit_transform(all_data[ord_features])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:18:29.614077Z","iopub.execute_input":"2022-04-26T14:18:29.614352Z","iopub.status.idle":"2022-04-26T14:18:29.790786Z","shell.execute_reply.started":"2022-04-26T14:18:29.614293Z","shell.execute_reply":"2022-04-26T14:18:29.78991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge encoded and scaled features with csr format\nfrom scipy import sparse\n\nall_data_sprs = sparse.hstack([sparse.csr_matrix(all_data), # Return all_data to CSR format\n                              encoded_nom_matrix,\n                              encoded_date_matrix],\n                              format='csr')\n\n# Split data into train data and test date\nnum_train = len(train)\nX_train = all_data_sprs[:num_train]\nX_test = all_data_sprs[num_train:]\ny = train['target']","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:18:29.792055Z","iopub.execute_input":"2022-04-26T14:18:29.792446Z","iopub.status.idle":"2022-04-26T14:18:30.603462Z","shell.execute_reply.started":"2022-04-26T14:18:29.792413Z","shell.execute_reply":"2022-04-26T14:18:30.602696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3) Make evaluation index calculation function","metadata":{}},{"cell_type":"markdown","source":"#### 4) Hyperparameter optimization (Train model)","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n# Generate model\nlogistic_model = LogisticRegression()\n# List of hyperparameter values (Dict type)\nlr_params = {'C':[0.1, 0.125, 0.2], 'max_iter':[800, 900, 1000],\n            'solver':['liblinear'], 'random_state':[42]}\n# Generate GridSearch object\ngridsearch_logistic_model = GridSearchCV(estimator=logistic_model,\n                                        param_grid=lr_params,\n                                        scoring='roc_auc',\n                                        cv=5)\n# Train model and GridSearch\ngridsearch_logistic_model.fit(X_train, y)\n\nprint(f'best hyperparameter :', gridsearch_logistic_model.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:18:56.117814Z","iopub.execute_input":"2022-04-26T14:18:56.118116Z","iopub.status.idle":"2022-04-26T14:29:55.099104Z","shell.execute_reply.started":"2022-04-26T14:18:56.11809Z","shell.execute_reply":"2022-04-26T14:29:55.098045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5) Validate performance","metadata":{}},{"cell_type":"markdown","source":"#### 6) Submit","metadata":{}},{"cell_type":"code","source":"# Predict with test data\ny_preds = gridsearch_logistic_model.best_estimator_.predict_proba(X_test)[:, 1]\n\n# Save submission file\nsubmission['target'] = y_preds\nsubmission.to_csv('submission.csv')\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:29:55.101974Z","iopub.execute_input":"2022-04-26T14:29:55.1024Z","iopub.status.idle":"2022-04-26T14:29:55.652456Z","shell.execute_reply.started":"2022-04-26T14:29:55.102355Z","shell.execute_reply":"2022-04-26T14:29:55.651561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"References\n===\n- [EDA reference](https://www.kaggle.com/kabure/eda-feat-engineering-encode-conquer)\n- [Modeling reference](https://www.kaggle.com/dkomyagin/cat-in-the-dat-0-80285-private-lb-solution)\n- 머신러닝.딥러닝 문제해결 전략(신백균)","metadata":{}}]}